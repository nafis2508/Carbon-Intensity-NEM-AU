{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6857b06e-8ccf-444b-bb34-1f1b24beaa1c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef44fd79-4fdd-4e51-8dbd-8e5b60c4c4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: requests-oauth2client in /opt/anaconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: polars in /opt/anaconda3/lib/python3.12/site-packages (1.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from requests-oauth2client) (25.3.0)\n",
      "Requirement already satisfied: binapy>=0.8 in /opt/anaconda3/lib/python3.12/site-packages (from requests-oauth2client) (0.8.0)\n",
      "Requirement already satisfied: furl>=2.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from requests-oauth2client) (2.1.4)\n",
      "Requirement already satisfied: jwskate>=0.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests-oauth2client) (0.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from binapy>=0.8->requests-oauth2client) (4.11.0)\n",
      "Requirement already satisfied: six>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from furl>=2.1.2->requests-oauth2client) (1.16.0)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from furl>=2.1.2->requests-oauth2client) (1.0.1)\n",
      "Requirement already satisfied: cryptography>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from jwskate>=0.11.1->requests-oauth2client) (43.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=3.4->jwskate>=0.11.1->requests-oauth2client) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=3.4->jwskate>=0.11.1->requests-oauth2client) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests requests-oauth2client polars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228ec9f3-b5c0-45c6-9e24-40a05bc6307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import polars as pl\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "from requests_oauth2client import OAuth2Client, OAuth2ClientCredentialsAuth\n",
    "from typing import List\n",
    "\n",
    "# ✅ Your credentials\n",
    "CLIENT_ID = \"2d2258e8-bc34-4119-8431-d71ca1ae5cf3\"\n",
    "CLIENT_SECRET = \"H6n8Q~0ph-GF4ehM.uLgp6E~cemXSHP1UkfcAc1_\"\n",
    "\n",
    "class MyEmissionsData(requests.Session):\n",
    "    _auth_url = \"https://login.microsoftonline.com/a815c246-a01f-4d10-bc3e-eeb6a48ef48a/oauth2/v2.0/token\"\n",
    "    _senaps_url = \"https://senaps.eratos.com/api/sensor/v2/observations\"\n",
    "\n",
    "    def __init__(self, client_id: str = CLIENT_ID, client_secret: str = CLIENT_SECRET):\n",
    "        super().__init__()\n",
    "        oauth2client = OAuth2Client(self._auth_url, (client_id, client_secret))\n",
    "        self.auth = OAuth2ClientCredentialsAuth(oauth2client, scope=f\"{client_id}/.default\")\n",
    "        self.headers = {\n",
    "            \"accept\": \"*/*\",\n",
    "            \"content-type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def download_and_parse_data(self, *, write_path: Path, regions: List[str], start: str, end: str, limit: int = 99_999_999):\n",
    "        if len(regions) == 0:\n",
    "            raise ValueError(\"`regions` list cannot be empty\")\n",
    "        parser = self._parse_single_stream if len(regions) == 1 else self._parse_multiple_streams\n",
    "\n",
    "        streamid = \",\".join(\n",
    "            f\"csiro.energy.dch.agshop.regional_global_emissions.{region}\" for region in regions\n",
    "        )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            fname = Path(tmpdir) / \"response.json\"\n",
    "            with self.get(\n",
    "                url=self._senaps_url,\n",
    "                params=dict(streamid=streamid, start=start, end=end, limit=limit),\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(fname, \"wb\") as fp:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        fp.write(chunk)\n",
    "\n",
    "            write_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(fname, \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                parser(data, write_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_single_stream(data, write_path):\n",
    "        col_name = data[\"_embedded\"][\"stream\"][\"_links\"][\"self\"][\"id\"]\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\"timestamp\": elem[\"t\"], col_name: elem[\"v\"][\"v\"]}\n",
    "                for elem in data[\"results\"]\n",
    "            ])\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_multiple_streams(data, write_path):\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\n",
    "                    \"timestamp\": key,\n",
    "                    \"struct\": {k: v[\"v\"] for k, v in val.items()}\n",
    "                }\n",
    "                for elem in data[\"results\"]\n",
    "                for key, val in elem.items()\n",
    "            ])\n",
    "            .unnest(\"struct\")\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f5667-82ab-466b-8d4b-51c00055b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions = MyEmissionsData()\n",
    "\n",
    "emissions.download_and_parse_data(\n",
    "    regions=[\"nsw\", \"qld\"],\n",
    "    start=\"2024-01-01T00:00:00.000Z\",\n",
    "    end=\"2024-01-07T00:00:00.000Z\",\n",
    "    write_path=Path(\"./csiro_5min_emissions.parquet\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b2a14-55b1-4341-959c-1d772f48c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"csiro_5min_emissions.parquet\")\n",
    "df.write_csv(\"csiro_5min_emissions.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9932172-2260-45fc-a25d-a05c97debf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pl.read_parquet(\"csiro_5min_emissions.parquet\")\n",
    "\n",
    "# Use the same dataset_merge folder path\n",
    "folder_path = os.path.expanduser(\"~/Desktop/dataset_merge\")\n",
    "output_path = os.path.join(folder_path, \"csiro_5min_emissions.csv\")\n",
    "\n",
    "# Save the CSV file\n",
    "df.write_csv(output_path)\n",
    "\n",
    "print(f\"✅ File saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facbdb25-db4b-483b-8928-f647054dabb3",
   "metadata": {},
   "source": [
    "# Downloaded the DATASET for a single day of carbon emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8478f-2c04-4c7e-9c4f-0c588c03ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import polars as pl\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "from requests_oauth2client import OAuth2Client, OAuth2ClientCredentialsAuth\n",
    "from typing import List\n",
    "\n",
    "# ✅ Your credentials\n",
    "CLIENT_ID = \"2d2258e8-bc34-4119-8431-d71ca1ae5cf3\"\n",
    "CLIENT_SECRET = \"H6n8Q~0ph-GF4ehM.uLgp6E~cemXSHP1UkfcAc1_\"\n",
    "\n",
    "class MyEmissionsData(requests.Session):\n",
    "    _auth_url = \"https://login.microsoftonline.com/a815c246-a01f-4d10-bc3e-eeb6a48ef48a/oauth2/v2.0/token\"\n",
    "    _senaps_url = \"https://senaps.eratos.com/api/sensor/v2/observations\"\n",
    "\n",
    "    def __init__(self, client_id: str = CLIENT_ID, client_secret: str = CLIENT_SECRET):\n",
    "        super().__init__()\n",
    "        oauth2client = OAuth2Client(self._auth_url, (client_id, client_secret))\n",
    "        self.auth = OAuth2ClientCredentialsAuth(oauth2client, scope=f\"{client_id}/.default\")\n",
    "        self.headers = {\n",
    "            \"accept\": \"*/*\",\n",
    "            \"content-type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def download_and_parse_data(self, *, write_path: Path, regions: List[str], start: str, end: str, limit: int = 99_999_999):\n",
    "        if not regions:\n",
    "            raise ValueError(\"`regions` list cannot be empty\")\n",
    "        parser = self._parse_single_stream if len(regions) == 1 else self._parse_multiple_streams\n",
    "\n",
    "        streamid = \",\".join(\n",
    "            f\"csiro.energy.dch.agshop.regional_global_emissions.{region}\" for region in regions\n",
    "        )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            fname = Path(tmpdir) / \"response.json\"\n",
    "            with self.get(\n",
    "                url=self._senaps_url,\n",
    "                params=dict(streamid=streamid, start=start, end=end, limit=limit),\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(fname, \"wb\") as fp:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        fp.write(chunk)\n",
    "\n",
    "            write_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(fname, \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                parser(data, write_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_single_stream(data, write_path):\n",
    "        col_name = data[\"_embedded\"][\"stream\"][\"_links\"][\"self\"][\"id\"]\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\"timestamp\": elem[\"t\"], col_name: elem[\"v\"][\"v\"]}\n",
    "                for elem in data[\"results\"]\n",
    "            ])\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_multiple_streams(data, write_path):\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\n",
    "                    \"timestamp\": key,\n",
    "                    \"struct\": {k: v[\"v\"] for k, v in val.items()}\n",
    "                }\n",
    "                for elem in data[\"results\"]\n",
    "                for key, val in elem.items()\n",
    "            ])\n",
    "            .unnest(\"struct\")\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "# ✅ Run the extraction here\n",
    "if __name__ == \"__main__\":\n",
    "    e = MyEmissionsData()\n",
    "\n",
    "    e.download_and_parse_data(\n",
    "        regions=[\"nsw\", \"qld\", \"vic\", \"sa\", \"tas\"],  # 👈 include all regions here\n",
    "        start=\"2024-01-01T00:00:00.000Z\",            # 👈 adjust time range as needed\n",
    "        end=\"2024-01-02T00:00:00.000Z\",\n",
    "        write_path=Path.home() / \"Desktop\" / \"csiro_5min_emissions_full.parquet\"\n",
    "    )\n",
    "\n",
    "    print(\"✅ CSIRO data with all 5 regions downloaded and saved to Desktop.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee27620-bf55-4ccf-a159-a0c5e0289ce6",
   "metadata": {},
   "source": [
    "# Saved it into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756dd0b-71f6-4558-9485-c970dc4d067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the .parquet file from Desktop\n",
    "parquet_path = Path.home() / \"Desktop\" / \"csiro_5min_emissions_full.parquet\"\n",
    "df = pl.read_parquet(parquet_path)\n",
    "\n",
    "# Define the output path to your ~/Desktop/dataset_merge folder\n",
    "output_path = Path.home() / \"Desktop\" / \"dataset_merge\" / \"csiro_5min_emissions_full.csv\"\n",
    "\n",
    "# Save as CSV\n",
    "df.write_csv(output_path)\n",
    "\n",
    "print(f\"✅ CSV file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe49b2f-21e3-433d-81a2-c9a974e3d7a9",
   "metadata": {},
   "source": [
    "# Download & Parse Data for the whole year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b41498-ab91-460a-8712-6453d9ea99e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSIRO full-year data downloaded and saved to Desktop.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import polars as pl\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "from requests_oauth2client import OAuth2Client, OAuth2ClientCredentialsAuth\n",
    "from typing import List\n",
    "\n",
    "# ✅ Your credentials\n",
    "CLIENT_ID = \"2d2258e8-bc34-4119-8431-d71ca1ae5cf3\"\n",
    "CLIENT_SECRET = \"H6n8Q~0ph-GF4ehM.uLgp6E~cemXSHP1UkfcAc1_\"\n",
    "\n",
    "class MyEmissionsData(requests.Session):\n",
    "    _auth_url = \"https://login.microsoftonline.com/a815c246-a01f-4d10-bc3e-eeb6a48ef48a/oauth2/v2.0/token\"\n",
    "    _senaps_url = \"https://senaps.eratos.com/api/sensor/v2/observations\"\n",
    "\n",
    "    def __init__(self, client_id: str = CLIENT_ID, client_secret: str = CLIENT_SECRET):\n",
    "        super().__init__()\n",
    "        oauth2client = OAuth2Client(self._auth_url, (client_id, client_secret))\n",
    "        self.auth = OAuth2ClientCredentialsAuth(oauth2client, scope=f\"{client_id}/.default\")\n",
    "        self.headers = {\n",
    "            \"accept\": \"*/*\",\n",
    "            \"content-type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def download_and_parse_data(self, *, write_path: Path, regions: List[str], start: str, end: str, limit: int = 99_999_999):\n",
    "        if not regions:\n",
    "            raise ValueError(\"`regions` list cannot be empty\")\n",
    "        parser = self._parse_single_stream if len(regions) == 1 else self._parse_multiple_streams\n",
    "\n",
    "        streamid = \",\".join(\n",
    "            f\"csiro.energy.dch.agshop.regional_global_emissions.{region}\" for region in regions\n",
    "        )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            fname = Path(tmpdir) / \"response.json\"\n",
    "            with self.get(\n",
    "                url=self._senaps_url,\n",
    "                params=dict(streamid=streamid, start=start, end=end, limit=limit),\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(fname, \"wb\") as fp:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        fp.write(chunk)\n",
    "\n",
    "            write_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(fname, \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                parser(data, write_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_single_stream(data, write_path):\n",
    "        col_name = data[\"_embedded\"][\"stream\"][\"_links\"][\"self\"][\"id\"]\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\"timestamp\": elem[\"t\"], col_name: elem[\"v\"][\"v\"]}\n",
    "                for elem in data[\"results\"]\n",
    "            ])\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_multiple_streams(data, write_path):\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\n",
    "                    \"timestamp\": key,\n",
    "                    \"struct\": {k: v[\"v\"] for k, v in val.items()}\n",
    "                }\n",
    "                for elem in data[\"results\"]\n",
    "                for key, val in elem.items()\n",
    "            ])\n",
    "            .unnest(\"struct\")\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "# ✅ Run the extraction here\n",
    "if __name__ == \"__main__\":\n",
    "    e = MyEmissionsData()\n",
    "\n",
    "    e.download_and_parse_data(\n",
    "        regions=[\"nsw\", \"qld\", \"vic\", \"sa\", \"tas\"],  # 👈 All regions\n",
    "        start=\"2024-01-01T00:00:00.000Z\",            # 👈 Start of 2024\n",
    "        end=\"2025-01-01T00:00:00.000Z\",              # 👈 End of 2024\n",
    "        write_path=Path.home() / \"Desktop\" / \"csiro_5min_emissions_full.parquet\"\n",
    "    )\n",
    "\n",
    "    print(\"✅ CSIRO full-year data downloaded and saved to Desktop.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048947d-2225-4221-97e4-1ce64c1e773f",
   "metadata": {},
   "source": [
    "# Convert .parquet to .csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34dd22c6-cd68-4be7-b928-73e26699a050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file saved to: /Users/nafis/Desktop/dataset_merge/csiro_5min_emissions_full.csv\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the .parquet file from Desktop\n",
    "parquet_path = Path.home() / \"Desktop\" / \"csiro_5min_emissions_full.parquet\"\n",
    "df = pl.read_parquet(parquet_path)\n",
    "\n",
    "# Define the output path to your ~/Desktop/dataset_merge folder\n",
    "output_path = Path.home() / \"Desktop\" / \"dataset_merge\" / \"csiro_5min_emissions_full.csv\"\n",
    "\n",
    "# Save as CSV\n",
    "df.write_csv(output_path)\n",
    "\n",
    "print(f\"✅ CSV file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb89ea7-abba-4d02-a9a5-8ca84753e4ef",
   "metadata": {},
   "source": [
    "## Downloading the DATA from 2019 to 2024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9bd4d9c-804b-4275-9b83-0f5361af8b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 5-year CSIRO emissions data downloaded to Desktop.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import polars as pl\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "from requests_oauth2client import OAuth2Client, OAuth2ClientCredentialsAuth\n",
    "from typing import List\n",
    "\n",
    "# ✅ Your credentials\n",
    "CLIENT_ID = \"2d2258e8-bc34-4119-8431-d71ca1ae5cf3\"\n",
    "CLIENT_SECRET = \"H6n8Q~0ph-GF4ehM.uLgp6E~cemXSHP1UkfcAc1_\"\n",
    "\n",
    "class MyEmissionsData(requests.Session):\n",
    "    _auth_url = \"https://login.microsoftonline.com/a815c246-a01f-4d10-bc3e-eeb6a48ef48a/oauth2/v2.0/token\"\n",
    "    _senaps_url = \"https://senaps.eratos.com/api/sensor/v2/observations\"\n",
    "\n",
    "    def __init__(self, client_id: str = CLIENT_ID, client_secret: str = CLIENT_SECRET):\n",
    "        super().__init__()\n",
    "        oauth2client = OAuth2Client(self._auth_url, (client_id, client_secret))\n",
    "        self.auth = OAuth2ClientCredentialsAuth(oauth2client, scope=f\"{client_id}/.default\")\n",
    "        self.headers = {\n",
    "            \"accept\": \"*/*\",\n",
    "            \"content-type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def download_and_parse_data(self, *, write_path: Path, regions: List[str], start: str, end: str, limit: int = 99_999_999):\n",
    "        if not regions:\n",
    "            raise ValueError(\"`regions` list cannot be empty\")\n",
    "        parser = self._parse_single_stream if len(regions) == 1 else self._parse_multiple_streams\n",
    "\n",
    "        streamid = \",\".join(\n",
    "            f\"csiro.energy.dch.agshop.regional_global_emissions.{region}\" for region in regions\n",
    "        )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            fname = Path(tmpdir) / \"response.json\"\n",
    "            with self.get(\n",
    "                url=self._senaps_url,\n",
    "                params=dict(streamid=streamid, start=start, end=end, limit=limit),\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(fname, \"wb\") as fp:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        fp.write(chunk)\n",
    "\n",
    "            write_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(fname, \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                parser(data, write_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_single_stream(data, write_path):\n",
    "        col_name = data[\"_embedded\"][\"stream\"][\"_links\"][\"self\"][\"id\"]\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\"timestamp\": elem[\"t\"], col_name: elem[\"v\"][\"v\"]}\n",
    "                for elem in data[\"results\"]\n",
    "            ])\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_multiple_streams(data, write_path):\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\n",
    "                    \"timestamp\": key,\n",
    "                    \"struct\": {k: v[\"v\"] for k, v in val.items()}\n",
    "                }\n",
    "                for elem in data[\"results\"]\n",
    "                for key, val in elem.items()\n",
    "            ])\n",
    "            .unnest(\"struct\")\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "# ✅ Run the extraction here\n",
    "if __name__ == \"__main__\":\n",
    "    e = MyEmissionsData()\n",
    "    e.download_and_parse_data(\n",
    "        regions=[\"nsw\", \"qld\", \"vic\", \"sa\", \"tas\"],                           # All NEM regions\n",
    "        start=\"2019-01-01T00:00:00.000Z\",                                    # Start of 5-year period\n",
    "        end=\"2024-12-31T23:59:59.999Z\",                                      # End of 2024\n",
    "        write_path=Path.home() / \"Desktop\" / \"csiro_5y_emissions.parquet\"   # Output path\n",
    "    )\n",
    "    print(\"✅ 5-year CSIRO emissions data downloaded to Desktop.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572c2db-dc31-4973-9652-328b3082791e",
   "metadata": {},
   "source": [
    "## Converting to CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9fc9d2-2650-4b4e-9460-6654da442b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file saved to: /Users/nafis/Desktop/dataset_merge/csiro_5y_emissions.csv\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the .parquet file from Desktop\n",
    "parquet_path = Path.home() / \"Desktop\" / \"csiro_5y_emissions.parquet\"\n",
    "df = pl.read_parquet(parquet_path)\n",
    "\n",
    "# Define output path inside Desktop/dataset_merge\n",
    "output_path = Path.home() / \"Desktop\" / \"dataset_merge\" / \"csiro_5y_emissions.csv\"\n",
    "\n",
    "# Save to CSV\n",
    "df.write_csv(output_path)\n",
    "\n",
    "print(f\"✅ CSV file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a90e75-2aea-440d-bda9-3a842bc0d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSIRO data from 2019 to April 2025 downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import polars as pl\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "from requests_oauth2client import OAuth2Client, OAuth2ClientCredentialsAuth\n",
    "from typing import List\n",
    "\n",
    "# ✅ Your new credentials\n",
    "CLIENT_ID = \"5259460c-3aa8-421a-afc2-eefca36ab37e\"\n",
    "CLIENT_SECRET = \"~2s8Q~9b~pQ458c140PcH5mJFgKsspfW_6upCdAE\"\n",
    "\n",
    "class MyEmissionsData(requests.Session):\n",
    "    _auth_url = \"https://login.microsoftonline.com/a815c246-a01f-4d10-bc3e-eeb6a48ef48a/oauth2/v2.0/token\"\n",
    "    _senaps_url = \"https://senaps.eratos.com/api/sensor/v2/observations\"\n",
    "\n",
    "    def __init__(self, client_id: str = CLIENT_ID, client_secret: str = CLIENT_SECRET):\n",
    "        super().__init__()\n",
    "        oauth2client = OAuth2Client(self._auth_url, (client_id, client_secret))\n",
    "        self.auth = OAuth2ClientCredentialsAuth(oauth2client, scope=f\"{client_id}/.default\")\n",
    "        self.headers = {\n",
    "            \"accept\": \"*/*\",\n",
    "            \"content-type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def download_and_parse_data(self, *, write_path: Path, regions: List[str], start: str, end: str, limit: int = 99_999_999):\n",
    "        if not regions:\n",
    "            raise ValueError(\"`regions` list cannot be empty\")\n",
    "        parser = self._parse_single_stream if len(regions) == 1 else self._parse_multiple_streams\n",
    "\n",
    "        streamid = \",\".join(\n",
    "            f\"csiro.energy.dch.agshop.regional_global_emissions.{region}\" for region in regions\n",
    "        )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            fname = Path(tmpdir) / \"response.json\"\n",
    "            with self.get(\n",
    "                url=self._senaps_url,\n",
    "                params=dict(streamid=streamid, start=start, end=end, limit=limit),\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(fname, \"wb\") as fp:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        fp.write(chunk)\n",
    "\n",
    "            write_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(fname, \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                parser(data, write_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_single_stream(data, write_path):\n",
    "        col_name = data[\"_embedded\"][\"stream\"][\"_links\"][\"self\"][\"id\"]\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\"timestamp\": elem[\"t\"], col_name: elem[\"v\"][\"v\"]}\n",
    "                for elem in data[\"results\"]\n",
    "            ])\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_multiple_streams(data, write_path):\n",
    "        (\n",
    "            pl.LazyFrame([\n",
    "                {\n",
    "                    \"timestamp\": key,\n",
    "                    \"struct\": {k: v[\"v\"] for k, v in val.items()}\n",
    "                }\n",
    "                for elem in data[\"results\"]\n",
    "                for key, val in elem.items()\n",
    "            ])\n",
    "            .unnest(\"struct\")\n",
    "            .with_columns(\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.fZ\", strict=True)\n",
    "                .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "            )\n",
    "            .sort(\"timestamp\")\n",
    "            .sink_parquet(write_path)\n",
    "        )\n",
    "\n",
    "# ✅ Download data from 2019 to April 2025\n",
    "if __name__ == \"__main__\":\n",
    "    e = MyEmissionsData()\n",
    "    e.download_and_parse_data(\n",
    "        regions=[\"nsw\", \"qld\", \"vic\", \"sa\", \"tas\"],\n",
    "        start=\"2019-01-01T00:00:00.000Z\",\n",
    "        end=\"2025-04-30T23:59:59.999Z\",\n",
    "        write_path=Path.home() / \"Desktop\" / \"dataset_merge\" / \"Question 2\" / \"csiro_2019_to_2025.parquet\"\n",
    "    )\n",
    "    print(\"✅ CSIRO data from 2019 to April 2025 downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2ff6c9-078b-4994-9417-5372fc23d9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file saved to: /Users/nafis/Desktop/dataset_merge/Question 2/csiro_2019_to_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the .parquet file\n",
    "parquet_path = Path.home() / \"Desktop\" / \"dataset_merge\" / \"Question 2\" / \"csiro_2019_to_2025.parquet\"\n",
    "df = pl.read_parquet(parquet_path)\n",
    "\n",
    "# Define the output path for CSV\n",
    "output_path = Path.home() / \"Desktop\" / \"dataset_merge\" / \"Question 2\" / \"csiro_2019_to_2025.csv\"\n",
    "\n",
    "# Save as CSV\n",
    "df.write_csv(output_path)\n",
    "\n",
    "print(f\"✅ CSV file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc8539-7f39-4d2b-9392-cfc751588a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
